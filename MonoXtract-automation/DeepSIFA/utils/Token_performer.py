"""
Take Performer as T2T Transformer, code borrowd from T2T
"""
import math
import torch
import torch.nn as nn
import numpy as np

class Token_performer(nn.Module):
    def __init__(self, dim, in_dim, head_cnt=1, kernel_ratio=0.5, dp1=0.1, dp2 = 0.1):#dimï¼šè¾“å…¥ token çš„é€šé“ç»´ï¼ˆä¾‹å¦‚ Transformer é‡Œæ¯ä¸ª token çš„ç»´åº¦ï¼‰ã€‚in_dimï¼šæ¯ä¸ªâ€œå¤´â€çš„ç»´åº¦æˆ–è¦æ˜ å°„åˆ°çš„åŸºç¡€ç»´ã€‚head_cntï¼šæ³¨æ„åŠ›å¤´æ•°ï¼ˆè¿™é‡Œé»˜è®¤ 1ï¼‰ã€‚dp1 / dp2ï¼šä¸¤ä¸ª dropout æ¯”ä¾‹
        super().__init__()
        self.emb = in_dim * head_cnt # we use 1, so it is no need here         #å®šä¹‰æ³¨æ„åŠ›æ€»ç»´åº¦ï¼ˆå¤šå¤´æ‹¼èµ·æ¥çš„ç»´åº¦ï¼‰ã€‚å½“ head_cnt=1 æ—¶ï¼Œemb=in_dimã€‚
        self.kqv = nn.Linear(dim, 3 * self.emb)                                #è‹¥è®¾ç½®head=1ï¼Œç›¸å½“äºå½“å¤´è‡ªæ³¨æ„åŠ›ï¼Œä»¤ç‰Œç»´åº¦ä¸º320ç»´ï¼Œå…ˆÃ—3ï¼Œç„¶ååˆ†ä¸º3ä»½ï¼Œæ¯ä»½ç»´åº¦éƒ½æ˜¯320ç»´ï¼Œè¿™ä¸‰ä»½åˆ†åˆ«å»è®¡ç®—Qï¼ŒKï¼ŒVã€‚
        self.dp = nn.Dropout(dp1)                                              #æ³¨æ„åŠ›å—é‡Œçš„ dropoutï¼ˆæ”¾åœ¨ QK^T æƒé‡ä¸Šæˆ–è¾“å‡ºä¸Šï¼Œå…·ä½“çœ‹ forward é‡Œæ€ä¹ˆç”¨ï¼‰ã€‚
        self.proj = nn.Linear(self.emb, self.emb)                              #æ³¨æ„åŠ›è¾“å‡ºåçš„çº¿æ€§æŠ•å½±ï¼ˆæ ‡å‡† Transformer é‡Œçš„ out_projï¼‰ï¼Œå½¢çŠ¶ (B, N, emb) -> (B, N, emb)ã€‚
        self.head_cnt = head_cnt                                               #è®°å½•å¤´æ•°ã€‚æ³¨æ„ï¼šå½“å‰ä»£ç é‡Œæœªè§å¯¹å¤šå¤´æ‹†åˆ†/åˆå¹¶çš„æ˜¾å¼å®ç°ï¼ˆæ²¡çœ‹åˆ° view/reshape æˆ (B, N, head, dim_head) çš„æ­¥éª¤ï¼‰ï¼Œè¯´æ˜ç›®å‰å¤§æ¦‚ç‡æŒ‰â€œå•å¤´â€åœ¨è·‘
        self.norm1 = nn.LayerNorm(dim)                                         #ç¬¬ä¸€ä¸ª LayerNormï¼Œé€šå¸¸ç”¨äº Attn å‰/å çš„å½’ä¸€åŒ–ï¼ˆPreNorm æˆ– PostNormï¼Œå–å†³äº forwardï¼‰ã€‚
        self.norm2 = nn.LayerNorm(self.emb)                                    #ç¬¬äºŒä¸ª LayerNormï¼Œé€šå¸¸ç”¨äº MLP å‰/å çš„å½’ä¸€åŒ–ã€‚å› ä¸º MLP çš„è¾“å…¥/è¾“å‡ºé€šé“æ˜¯ embã€‚
        self.epsilon = 1e-8  # for stable in division                          #å°å¸¸æ•°ï¼Œåšé™¤æ³•å½’ä¸€åŒ–æ—¶é˜²æ­¢åˆ†æ¯ä¸º 0
        self.drop_path = nn.Identity()                                         #ç”¨æ¥æ”¾ Stochastic Depthï¼ˆæ®‹å·®è·¯å¾„éšæœºä¸¢å¼ƒï¼‰ã€‚ç°åœ¨æ˜¯ Identityï¼Œç›¸å½“äºä¸å¼€å¯ã€‚å¦‚æœä½ å‡†å¤‡æ”¯æŒ DropPathï¼Œå¸¸è§ä¼šæœ‰ä¸€ä¸ªç‡ drop_path_probï¼Œç„¶åç”¨è‡ªå®šä¹‰ DropPath æ¨¡å—æ›¿æ¢å®ƒã€‚

        self.mlp = nn.Sequential(                                              #Transformer çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ã€‚
            nn.Linear(self.emb, 1 * self.emb),                                 #å¯¹æ¯ä¸ª token çš„å‘é‡ ğ‘¥RD_inåšä¸€æ¬¡çº¿æ€§æŠ•å½±ï¼Œè¾“å…¥å’Œè¾“å‡ºç»´åº¦éƒ½æ˜¯embã€‚é€šå¸¸æ¥è¯´nn.Linear(self.emb, hidden)
            nn.GELU(),                                                         #ä¸€ä¸ªéçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œå®ƒçš„æ„ä¹‰/ä»·å€¼ä¸»è¦åœ¨äºï¼šè®©ç½‘ç»œåœ¨åšé€šé“æ˜ å°„æ—¶å¼•å…¥å¹³æ»‘ã€æ¦‚ç‡æ„Ÿæ›´å¼ºçš„éçº¿æ€§ï¼Œä»è€Œå…¼é¡¾ä¼˜åŒ–ç¨³å®šæ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚
            nn.Linear(1 * self.emb, self.emb),                                 #å†æ¬¡è¿›è¡Œä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼Œæ˜¯æŠŠç»è¿‡æ¿€æ´»åçš„â€œé«˜ç»´ç‰¹å¾â€å†å‹å›åŸç»´åº¦ï¼Œç”¨æ¥å’Œæ®‹å·®ç›¸åŠ 
            nn.Dropout(dp2),                                                   #è¿™ä¸€æ­¥ä¸æ˜¯â€œå¿…é¡»è®©çº¿æ€§æ›´å‡†â€ï¼Œè€Œæ˜¯é˜²è¿‡æ‹Ÿåˆç”¨çš„
        )

        self.m = int(self.emb * kernel_ratio)#å®šä¹‰éšæœºç‰¹å¾ä¸ªæ•° mï¼ˆPerformer é‡Œçš„æ ¸è¿‘ä¼¼ç»´åº¦ï¼‰ã€‚kernel_ratio=0.5 â†’ m â‰ˆ 0.5*embã€‚m è¶Šå¤§ â†’ è¿‘ä¼¼è¶Šå‡†ï¼Œä½†ç®—å¾—æ›´æ…¢æ›´å æ˜¾å­˜ã€‚
        self.w = torch.randn(self.m, self.emb)#é‡‡æ ·ä¸€ä¸ªéšæœºçŸ©é˜µ W âˆˆ â„^{mÃ—emb}ï¼Œç”¨äºæŠŠ Q,K æŠ•åˆ°éšæœºç‰¹å¾ç©ºé—´ï¼ˆÏ†(Q)=f(QW^T,...)ï¼‰ã€‚
        self.w = nn.Parameter(nn.init.orthogonal_(self.w) * math.sqrt(self.m), requires_grad=False)#ç”¨ æ­£äº¤åˆå§‹åŒ–ï¼ˆorthogonalï¼‰è®© W çš„è¡Œå‘é‡ä¸¤ä¸¤æ­£äº¤ï¼Œæ•°å€¼æ›´ç¨³ï¼Œä¹˜ä¸Š âˆšm è¿›è¡Œå°ºåº¦è°ƒæ•´ï¼ˆå¸¸è§å®ç°é‡Œä¼šåœ¨æ˜ å°„é‡Œæœ‰ 1/âˆšm çš„å½’ä¸€åŒ–ï¼›è¿™é‡Œå…ˆä¹˜ âˆšmï¼Œåç»­å¤šåŠä¼šé…åˆé™¤æ³•/å½’ä¸€æ­¥éª¤æŠµæ¶ˆï¼Œä¿æŒæ–¹å·®åœ¨åˆé€‚é‡çº§ï¼‰ï¼Œrequires_grad=Falseï¼šä¸è®­ç»ƒè¿™ä¸ªçŸ©é˜µï¼ˆéšæœºç‰¹å¾å›ºå®šï¼‰

    def prm_exp(self, x):                                                      #å’Œ MHSA çš„å…³ç³»ï¼šå®ƒæ˜¯ MHSA å†…éƒ¨â€œç®—æ³¨æ„åŠ›â€çš„ä¸€ç§è¿‘ä¼¼æ›¿ä»£ï¼ˆçº¿æ€§æ³¨æ„åŠ›ï¼‰ï¼Œå±äºæ³¨æ„åŠ›åˆ†æ”¯çš„ä¸€éƒ¨åˆ†ï¼Œæ ‡å‡† MHSAï¼šsoftmax(QKáµ€)Vï¼ˆO(NÂ²)ï¼‰ï¼ŒPerformer-MHSAï¼šç”¨ Ï† è¿‘ä¼¼åæŒ‰ä¸Šå¼çº¿æ€§ç®—ï¼ˆO(Nm)ï¼‰
        # part of the function is borrow from https://github.com/lucidrains/performer-pytorch 
        # and Simo Ryu (https://github.com/cloneofsimo)
        # ==== positive random features for gaussian kernels ====
        # x = (B, T, hs)
        # w = (m, hs)
        # return : x : B, T, m
        # SM(x, y) = E_w[exp(w^T x - |x|/2) exp(w^T y - |y|/2)]
        # therefore return exp(w^Tx - |x|/2)/sqrt(m)
        xd = ((x * x).sum(dim=-1, keepdim=True)).repeat(1, 1, self.m) / 2
        wtx = torch.einsum('bti,mi->btm', x.float(), self.w)

        return torch.exp(wtx - xd) / math.sqrt(self.m)               #æ ‡å‡† MHSAï¼šå¯¹æ¯ä¸ª queryï¼Œè¦å’Œæ‰€æœ‰ key åšç›¸ä¼¼åº¦ï¼ˆNÃ—Nï¼‰ï¼Œæˆæœ¬é«˜ã€‚
#Performerï¼šå…ˆæŠŠ Q/K æŠ•åˆ° m ç»´éšæœºåŸºåº•å¹¶åšæ­£å€¼åŒ– â†’ åªåœ¨è¿™ä¸ª m ç»´é‡Œåšä¸¤æ¬¡ä¹˜æ³•ï¼Œåƒæ˜¯æŠŠâ€œå…¨å±€ä¸¤ä¸¤ç›¸ä¼¼åº¦â€æ”¹å†™æˆâ€œå…ˆå‹ç¼©åˆ° m ç»´ï¼Œå†åœ¨ m ç»´é‡Œèšåˆâ€ã€‚
    def attn(self, x):                                                              # [8, 16384, 256]
        k, q, v = torch.split(self.kqv(x), self.emb, dim=-1)
        kp, qp = self.prm_exp(k), self.prm_exp(q)  # (B, T, m), (B, T, m)
        D = torch.einsum('bti,bi->bt', qp, kp.sum(dim=1)).unsqueeze(dim=2)  # (B, T, m) * (B, m) -> (B, T, 1)
        kptv = torch.einsum('bin,bim->bnm', v.float(), kp)  # (B, emb, m)
        y = torch.einsum('bti,bni->btn', qp, kptv) / (D.repeat(1, 1, self.emb) + self.epsilon)  # (B, T, emb)/Diag
        # skip connection
        y = v + self.dp(self.proj(y))  # same as token_transformer, use v as skip connection#æœ€åæ®‹å·®ç›¸åŠ 

        return y

    def forward(self, x):
        x = self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))#æœ€åæ®‹å·®ç›¸åŠ 
        return x
